# DroidTask

Dataset and evaluator of AutoDroid

You can get access to our [homepage](https://autodroid-sys.github.io) and [paper](https://arxiv.org/abs/2308.15272) for more details.

Download all the data to be used from [Google Drive](https://drive.google.com/drive/folders/1SergVsuQQThuhmRl63CtkQ5Vv-Pj68OQ?usp=share_link). Besides, we provide additional 
data in the `data` folder.

## Environment Setup
```
conda create -n droidtask python=3.10
pip install -r requirements.txt
```

or:

```
conda env create --name droidtask --file environment.yml
```

## User Task YAMLs

```
user_tasks
├── applauncher
│   ├── task1.yaml
│   ├── task2.yaml
│   ├── task3.yaml
│   ├── task4.yaml
│   └── task5.yaml
├── calendar
│   ├── task1.yaml
│   ├── task2.yaml
│   ├── task3.yaml
│   ├── task4.yaml
│   ├── task5.yaml
│   ├── task6.yaml
│   ├── task7.yaml
│   ├── task8.yaml
│   └── task9.yaml
```
there may be also other subfolders in each app folder such as `events`, etc. They are
generated by autodroid. You can get all information recorded by AutoDroid in these
additional files.

## Explore & Memory Generation
**Notice**: We have provided our generated `.json` file. If you want
to directly use our `data/ex_mem.json` and memory, you can skip this part.
```
utgs
├── applauncher
│   └── utg.yaml
├── calendar
│   └── utg.yaml
```
For the convenience of your direct usage, we keep raw files in the folder. If you
want to create your memory on your own, you need to extract all the `utg.yaml`, and
rename them into `${app}.yaml`, putting them in a `explorations_data` folder.

```python
# run_explorator.py
# example on explorator.py usage, remember to specify API keys in `utils.py`
from .explorator import Explorator

ex = Explorator(
    data_folder="data/exploration_data",
    output_path="data/ex_mem_new.json"
)

ex.loadMemory(tasks_path="data/user_tasks/")
```
in case there may be network problem when you execute code, we provide a `backup_mode`
in the `explorator` code, which will save essential data each step, you can check the 
code and adjust your save path if needed.

## Evaluator
You can evaluate LLM's answers through this code.

```python
# run_evaluator.py
# example on evaluator.py usage, remember to specify API keys in `utils.py`
from .evaluator import Evaluator

ev = Evaluator(
    data_folder="data/user_tasks",
    log_root="logs",
    memory_folder="data/navigation",
    use_memory=True,
    prompt_config="data/prompt.yaml",
    external_mem="data/ex_mem.json",
    instance_name="<set-a-name-by-yourself>",
    specify_app=["calendar", "clock", "contacts", "dialer", "messenger"],
)

ev.evaluate()
```

Some explanations on the parameters:
- `data_folder`: `utgs` folder path.
- `log_root`: log folder.
- `use_memory`: bool, whether to integrate memory.
- `memory_folder`(only if `use_memory`): `navigation` folder path; we provide our data in `data/navigation`, which is similar to `UI_function_table` in the paper; each key represents a `state_str`
of one UI state, and values represent next UI state's functions when operating on these indices of
elements.
- `prompt_config`: configuration file for prompts design. Default to `data/prompt.yaml`
- `use_baseline`: bool, whether to use baseline methods.
- `external_mem`: `.json` file generated by `explorator`. also provided in download link for directly
usage (`ex_mem.json`).``
- `instance_name`: subfolder to store log information for current run below `log_root`.
- `specify_app`: a list, you can choose to just evaluate some of the apps in the `data_folder`, such as ['applauncher', 'calendar'], etc.
- `load_result`: without querying LLMs, just load the result from former logs, when using this
option, set the `instance_name` same as the logs you want to load from.

### Notice

Because LLM would sometimes not output in the format we design strictly, we enable
the code to let you parse the unformatted output yourself, instead of just leaving them out.

For example, we ask LLM to output like:

`- id=<id number> - action=<tap/input> - input text=<text or N/A>`

but it may answer with "The id is xxx, the action is xxx, and no text is needed." (sometimes maybe more complex and hard to predict)

So when `Evaluator` find out the answer is not the format we want, it will **print the raw answer to the console** and let you read it, so that you can judge what the LLM really want to say.

Assume the example below:

`The id is 2, the action is tap, and no text is needed.`

The `Evaluator` would ask you to *"Please input id, action, and text: "*
- for `id`, you just type 2, that is fine;
- for `action`, we modify the code here, so you will not need to input `t-a-p`. Instead, you should type the index of `["tap", "input", "N/A"]`. In this example, the number is 0. 
    - For smaller models such as GPT-3.5 or Vicuna, there are quite a lot "mis-answers" like this.
- for `input_text`, if no text is needed, type -1; otherwise you type the text itself.
- To sum up, you should answer: `2, 0, -1` in this example.
